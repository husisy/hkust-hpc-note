# ws05 torch Distributed Data Parallel training (DDP)

submit the job

```bash
tcloud submit
```

see the output

```bash
tcloud cat slurm_log/ws05_torch_ddp.log
```

`ws05_torch_ddp.log`

```txt
# demo_basic[rank=1]
# demo_basic[rank=0]
```
